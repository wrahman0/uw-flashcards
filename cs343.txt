Chapter 6
=========

Lock Programming
================

* What is buffering?

When tasks communicate using a queue. Consumers pick up tasks in front of the
queue while producers put tasks at the back.

* How do you control access when using a buffer?

Have a counting semaphore. Initialize it to 0. When producers place thing in,
increase the number by one, ie .V(). When consumers take it, reduce it by 1, ie
.P().

Sample:
void Producer::main(){
	for (;;){
		// prod
		sem.V();
	}
}

void Consumer::main(){
	for(;;){
		sem.P();
		// take an item
	}
}

* Is there a problem when we add/remove from a shared queue?

Yes. Lets say sem = 3 and 2 consumers acquires the lock. They can both remove an
item from the queue. If they go at the same time, they take the same item and
cause issues because it seems like we consumed two items. However, in reality,
we processed just one.

* Is the semaphore above used for ME or sync?

Used for ME because it controls how/who is allowed into the critical section at
any given time.

* Whats bounded vs unbounded?

Bounded has finite length, unlike unbounded.

* What additional mechanism is needed for bounded queue?

We need an additional counting semaphore. The previous was for figuring when the
consumers needed to wait. We now need one when the producers need to wait.

void Producer::main(){
	for (;;){
		sem2.P(); // if the buffer is full, wait
		// prod
		sem.V();
	}
}

void Consumer::main(){
	for(;;){
		sem.P();
		// Took an item, thus, there is one empty slot. Signal prod
		sem2.V();
	}
}

Locking Techniques
==================

* What is a split binary semaphore?

A collection of semaphores where at most one is 1, others are 0. Used when
different tasks block based on their own independent criteria.

* What is baton passing?

Conceptual idea that helps you understand what goes on in the case of split
binary semaphores. Rules are that:

- One baton
- No task moves in the exit/entry code unless they have the baton
- Once baton released, cannot view variables in the entry/exit regions

- Can mutex/condition locks perform baton passing to prevent barging?

Lets say a task is waiting on a condition lock. It gets signalled but the
signaller is still going because they haven't released the lock. The waiting
task is still waiting, and there could be a race with a new task. If the race is
won by the new task, then it just barged in front of the waiting task. Thus,
no.

* What is the reader/writer problem?

Multiple tasks sharing a resource. Some reading others writing. We want to allow
concurrent readers but only one writter.

Using split binary semaphore, we split arrivers, readers and writers.

uSemaphore entry_q(1), read_q(0), write_q(0);
int r_waiting = 0, w_waiting = 0, r_count = 0, w_count = 0;

void Reader::main(){
	// Need access to the entry location since only one task has baton
	entry_q.P();
	if ( w_count > 0 ) { // we have active writer
		r_waiting += 1;
		entry_q.V(); // release the baton
		read_q.P(); // block on the read queue
	}

	r_count += 1; // we are now able to read.

	if ( r_waiting > 0 ) {
		// signal a waiting reader since reading has begun
		r_waiting -= 1;
		read_q.V();
	} else {
		// last waiting reader needs to open up the entry code to both
		// reader and writer
		entry_q.V();
	}

	yield(); // simulate a read

	entry_q.P(); // After done, we need baton to move in the exit code

	r_count -= 1; // we just left, thus one less reader
	if ( r_count == 0 && w_waiting > 0 ) {
		// if writers are waiting, let them in
		w_waiting -= 1;
		write_q.V();
	} else {
		// Otherwise open to all
		entry_q.V();
	}
}

void Writer::main() {
	// Need access to the baton for entry code
	entry_q.P();
	// We need to wait if there is reader OR writer in the room
	if ( r_count > 0 || w_count > ) {
		w_waiting += 1;
		entry_q.V(); // release the baton
		writer_q.P(); // Wait on the writer bench
	}

	w_count += 1;
	entry_q.V(); // release the baton

	if ( r_waiting > 0 ) {
		r_waiting -= 1; read_q.V();
	} else if ( w_waiting > 0 ) {
		w_waiting -= 1; write_q.V();
	} else { entry_q.V(); } // release the baton
}

* What is the problem?

Writers can be starved.

* Whats the solution to starvation?

If we add a || w_waiting > 0 clause in the reader guard, then readers can now
starve.

Use Dekker's algo to alternate when there are ties. If reader goes once, then
next time, force the writer to go. Problem is now staleness and freshness. It
could be that the writer needs to update the values but cant go because writers
are blocked. Meanwhile, readers reading stale data. If there are consecutive
writers then the data is too fresh.

* What is temporal ordering?

Temporal ordering is when the readers and writers are served in FIFO ordering.
We need one semaphore for that otherwise we dont distinguish between reader vs
writer arrival times.

We can use a shadow queue to get the type of task info. For instance, we can
have a shadow queue of:

Shadow queue:
Reader -> Writer -> Writer

Semaphore:
Task -> Task -> Task

* Whats the solution to the reader writer temporal problem?

Solution 5: Have a writer chair. If chair is empty, then unblock tasks in
reader/writer sem. If a writer is unblocked, place on chair.

Solution 6:
===========

Situation
For writer:
- Pick up baton, check reader is using resource
- Put down the baton, entry.V(), then gets time sliced before wait X_p.P()
- Another writer does the same thing
- Writers start at any order => freshness problem

Situation
For reader:
- Pick up baton and see if writer using resource
- Put down the baton, entry.V(), then gets time sliced.
- Happens multiple times => resumption is any order

Thought:
We can have atomic block and release or ticket method
X.P(entry_q); //uC++ semaphore

Ticket solution:
- Reader/writer take ticket before putting baton down
- To pass baton, serving counter is incremented and then wake all blocked
- Each woken one checks to see if their ticket is being served
- No starvation since waiting queue is bounded length and you will be served
- Inefficient AF

Private semaphore solution:
- List of private semaphores, one for each waiting task
- Add to list of nodes before entry.V() then block on private semaphore
- To pass baton V on the private semaphore head if present

Chapter 7
=========

Race Conditions
===============

* What is a race condition and when do they occur?

Race condition occurs when there is a missing sync or mutual exclusion. Two
tasks continue with the assumption that sync took place and depending on
execution speed for each run, outcome will vary and is undeterministic.

Live-lock
=========

* What is livelocking?

Its the indef postponement. Each task says: "You go first" and no one gets to
go.

* What causes live-locking?

It is caused by poor scheduling in entry protocol

Starvation
==========

* What is starvation?

A waiting task never gets serviced due to the selection algorithm

** Deadlocking

* What is deadlocking?

A state when one or more process is waiting for an event that will never occur.

* What is the difference between livelocking and deadlocking?

In livelocking, the tasks are always changing states and checking each other for
an update. The CPU is maxed out during this process. In deadlocking, the task is
waiting on some sort of lock and never gets serviced. As a result, the CPU does
no work and idles.

* What is sync deadlock?

Sync deadlock is failure in cooperation. A waiting task is waiting on the
cooperation to finish but never happens.

* What is ME deadlock?

When a task waits on a resource and is unable to acquire a resource thats locked
in ME.

* What are the 5 conditions for deadlock?

1. There is one or more shared resource requiring ME
2. A process goes to sleep while holding a resource (hold and wait)
3. The runtime sys cant forcefully get a held resource of a waiting task
4. There is a circular wait of processes on resources
5. These conditions happen simultaneously

Neat idea: since we need all 5, prevent one and you have no deadlock

* Deadlock Prevention

** Sync Prevention

* How do you prevent sync deadlock?

Get rid of all sync in a program => no communication. All tasks are indep

* How do you prevent ME deadlock?

1. No ME => not realistic
2. No hold and wait. Only give all res or none.
	- Poor res util
	- Starvation possible when a resource set is never avail and someone waits for
		it
3. Allow preemption for runtime sys
4. Dont have circ wait
5. Prevent simultaneous occurrence

* How do you prevent circular wait on a resource:

1. Divide resource to classes R1 ... Rn.
2. Cannot request resource from a class Ri if not holding res from Rj, j >= i.
	=> Have to request resources in increasing order of category number

* Deadlock avoidance

* Bankers algorithm

Show that a safe resource allocation path exists

Allocation graphs
=================

When a resource is allocated, draw line from process to resource.

* How can you determine deadlocks from a graph?

No directed cycle

* How do you use graph reduction to locate deadlocks?

1. Create a graph where resources are duplicated when multiple tasks need it
2. Find a task with only inwards arrows ( it has all the resources )
3. Delete the task ( completed )
4. Flip resource arrows to processes that need it
5. Repeat.

If you can finish, no deadlock. If you get stuck, deadlock

Detection and recovery
======================

* What do you need for deadlock recovery?

Preemption

Chapter 8
=========

Critical Regions
================

* How can language help with critical regions?

Provide REGION statements

* How can nesting result in deadlocks?

VAR x, y: SHARED INTEGER

task1:
REGION x DO
	REGION y DO

	END REGION
END REGION

task2:
REGION y DO
	REGION x DO

	END REGION
END REGION

* Why is simult reads impossible?

Lock needed before any read is done

* Why is REGION statement better?

Compiler can catch it

* Why is this bad?

Since we allow reads from the outside without REGION statements, it can seem
that values are flickering while reading because some task is updating those
values.

* Conditional Critical Regions

REGION v DO
	AWAIT conditional-expression

END REGION

If condition is false, we release the lock and busy wait on the lock

* Monitor

* What is a monitor?

Monitors ensure that shared data and public members are modified in a serial
fashion.

* What is a mutex member?

A member function that allows one active mutex member. There is mutexlock that
is acquired in entry and released when leaving or exception. Recursive entry is
allowed by using a multi acquistion lock.

* What is a destructor?

Destruction is blocked if the thread is active since the entire _Monitor is a
mutex.

* Scheduling ( Sync )

Monitors may need to schedule tasks in a different order than that of arrival.

* What are the types of scheduling?

- External (outside the monitor) using _Accept
- Internal (inside the monitor) using uCondition (benches)

External Scheduling
===================

* How does _Accept statements work?

There is a chair for each accept statement. When the new function calls are
coming in, they check the chairs. If a chair has the same function, we dont
accept those method calls. When the method that it accepted on exits or waits,
the waiting task then goes on.

* How can you accept on a chain on accept calls?

Change chair to a stack

Internal Scheduling
===================

Using a condition lock, we place calls on a waiting queue. When the
task is signalled, it still waits until the signaller exits or waits.

* When do you use internal vs external?

External => Use external scheduling when possible. Cannot use when we need data
from outside to decide when to block/unblock.

Internal => When we want a special ordering of method calls.

Reader/Writer Problem
=====================

* Stalenss in solution p137 but no freshness. Why?

Solution on pg 137 has staleness because the statement:

void endWrite(){
	wcnt = 0;
	if (!readers.empty()) readers.signal();
	else writers.signal();
}

We are prioritizing the readers. If its the other way around, we would freshness.

* Can the monitor read/write have the actual read/write functions in them?

For read: no because we will have no concurrency. One read at a time = stupid
For write: yes because we allow one writer at a time. Monitors will block others.

* What is the problem with this interface?

Forget the endRead() by the dev.

Alternative:

_Monitor ReadersWriter {
	_Mutex void startRead();
	...
	public:
	_Nomutex void read(...) {
		startRead();
		// read
		endRead();
	}
	...
}

Must be a _Nomutex, otherwise only one reader is allowed at a time.

* Using shadow queue to get temporal ordering: (Using shadow queue)

_Monitor ReadersWriter{
	int rcnt, wcnt;
	uCondition RW; // one bench for both
	enum RWenum { READER, WRITER };

	public:
	void startRead() {
		// if writer active or bench not empty, wait because we have to wait for our
		// turn to read
		if (wcnt != 0 || !RW.empty()) { RW.wait( READER ); }
		rcnt += 1;
		// if the front is a reader, then allow them to go. Otherwise, we have to
		// wait for the writer to update the values to prevent stale reads. If we
		// keep allowing readers before allowing that writer, then there is
		// staleness issue.
		if (!RW.empty() && RW.front() == READER) RW.signal();
	}

	// Nothing tricky here
	void endRead() { ... }
	void startWrite() { ... }
	void endWrite() { ... }
}

* Fix to all the issues:

Use external scheduling with _Accept().

* What is the nested monitor problem?

Say you are in Monitor X and you call Monitor Y. You have both X and Y's lock.
If you block inside Y, you implicitly release Y's lock. However, you are still
holding X's lock. If the cooperation to release you requires access to X's
methods, then its a deadlock because no one can enter X.

Condition, Signal, Wait vs Counting Sem, V, P
=============================================

* Whats the differences?

- Wait always blocks but P blocks if sem = 0
- signal before wait is lost, V before P isnt
- Multiple Vs can start multiple functions at once. Multiple signal wakes one at
	a time because tasks must exit the monitor sequentially

Monitor types
=============

* What is explicit scheduling vs implicit?

Explicit: When you as the dev have to schedule manually via accepts / signals
Implicit: When the monitor tasks care of sched for you. Monitors can prioritize
based on calling (C), signalled (W) and signaller (S) queues.

Possibile implicit priorities:
see pg141

* Why do we reject some priorities?

We reject all priorities where the calling task is the highest priority. This
causes starvation since an endless calling will not allow S/W to be serviced.

* How does implicit signalling work?

There is a waitUntil busy wait on the condition.

Example
_Monitor BoundedBuffer {
	public:
	...
	void insert ( ... ) {
		// Busy wait
		waitUntil count != 20; // not in uC++
		...
	}
}

* What are the types of blocking?

Blocking
C < S < W - Priority blocking because after W, S goes. uC++'s signalBlock()
C = S < W - No Priority blocking because after W, S or C could go

Non blocking
C < W < S - Priority nonblocking. Once W is signalled by S, S keeps running
C = W < S - No Prior nonblocking. Once S is done, W or C can go

Quasi blocking
C < W = S - When W is signalled, W or S can go
C = W = S - When W is signalled anyone can go

Immediate Return
C < W - After W is signalled by S, S is done immediately and W runs
C = W - W or C can go

Implicit Signal
C < W - After W is signalled, by the immediate return signal rule, S is done
C = W - C or W can go

* Pros and cons of signal types

Implicit - Good prototyping, bad perf

Immediate return - Not powerful for all cases. Optimizes for the common case of
signal before return.

Quasi - Make cooperation too difficult because a lot of condition checks are
needed

Priority Nonblocking - no barging and optimizes for signal before return. Just
means that if a signal is before a return, it will finish the signaller. If it
was blocking, then the signaller could have easily ended but now gets put on a
queue. Super inefficient.

Priority-blocking - no barging

No-priority non blocking - requires signalled task to recheck the waiting
condition in case of barging task => use while around wait instead of if

No-priority blocking - requires signaller to recheck condition

Java Monitors
=============

* What is synchronized in java?

_Mutex

* What is the blocking type used in java monitors?

No-priority nonblocking

* Whats the issue here:

p144
class Barrier {
	...
	public synchronized void block() {
		count ++;
		if ( count < N ) {
			try { wait (); } catch ( InterruptedException e ) {}
		} else {
			notifyAll();
		}
		count --;
	}
}

Issue is that when the notifyAll is called, one task may go and set the counter
down by one. Another task can go inside the block() and increase the counter by
one and see that its back to N and it notifies all again. Thats an issue because
the previous batch was still being processed.

* How do you fix it?

class Barrier {
	...
	public synchronized void block() {
		count ++;
		if ( count < N ) {
			try { wait (); } catch ( InterruptedException e ) {}
		} else {
			count = 0; // add this
			notifyAll();
		}
	}
}

* Why is it an issue with spurious wakeup?

Note, spurious wake ups dont exist.

Use ticket counter. p145
class Barrier { // monitor
	private int N, count = 0, generation = 0;
	public Barrier( int N ) { this.N = N; }
	public synchronized void block() {
		int mygen = generation;
		count += 1; // count each arriving task
		if ( count < N ) // barrier not full ? => wait
			while ( mygen == generation )
				try { wait(); } catch( InterruptedException e ) {}
		else { // barrier full
			count = 0; // reset count
			generation += 1; // next group
			notifyAll(); // wake all barrier tasks
		}
	}
}

* What is the issue with building condition variables in java with nested
*	monitors?

Deadlock because you release the lock of the immediate sync method but still
hold the lock of the outer sync method(s).

Chapter 9

Direct Communication
====================

* What are the different ways to communicate between tasks?

- Indirect comm: Tasks use monitor to store the input/output of their
	communication
- Direct comm: Tasks directly talk to each other

Task
====

* What is a task?

Task is like a coroutine becaus it has a main. Main has its own state and a
thread begins work in the main as soon as a task is created. Task also provides
ME so only one active thread is allowed in the object. It has its own thread and
stack and can be used for S/ME.

* List of classes and capabilities in uC++:

Thread  Stack  No S/ME   S/ME
-----------------------------------
NO			NO		 class		 monitor
NO			YES		 corout.	 cormonitor
YES			NO		 reject		 reject				// Not possible because uses caller's stk
YES			YES		 reject?	 task

Scheduling
==========

* In a task, why are _Accept statements moved inside of the main instead of the
* indiv routines?

The main function runs until it exits. As a result, no other calls are accepted
if _Accept is not placed in the main().

* Sample _When and _Accept stmt:

_Task BoundedBuffer {
	... 
	void main() { 
		for ( ;; ) {
			_When (count != 20) _Accept(insert) { 
				// code after insert finishes
			} or _When (count != 0) _Accept(remove) {
				// code after remove finishes
			}
		}
	}
}


* What is _Accept( m1, m2 ) s1 equiv to?

_Accept (m1) s1; or _Accept (m2) s1;

* What happens if no _When clause is satisfied?

Behaves like a switch stmt with no match. It keeps going and doesnt block.

* What happens when multiple clauses are satisfied?

Chooses the first _Accept satisfied. Ordering of _Accepts matter.

* Do the above code block of main() with if statments instead of _When.

	void main() { 
		for ( ;; ) {
			if ( count > 0 && count < 20) _Accept(insert, remove);
			else if ( count == 0 ) _Accept(insert);
			else if ( count == 20 ) _Accept(remove);
		}
	}

* How many if statements are needed for N accept clauses?

2^N - 1

* When does an _Accept stmt complete?

When the accepted method is done or a wait occurs inside of it.

* What is the _Else clause?

Using the _Else clause, if no accept can be executed immediately, the
terminating else is executed. For instance, if we need to check if the
destructor has been called but dont want to block on it.

* What happens when exception thrown in the members?

Propagates to the caller and a special event is raised in main() 
as a uMutexFailure::RendezvousFailure().

try {
	... _Accept(mem);
} catch (uMutexFailure::RendezvousFailure) {
	...
}

Note: _Enable is implicit for RendezvousFailure.

Accepting Destructor
====================

* Why is accepting a destructor different?

When a caller makes a destruction call, it blocks immediately because
deallocations occur in destructors. We dont want to dealloc storage and then do
the _Accept { /* things in here */ }. Thus, the "things in here" runs
immediately and then the destructor call procedes. 

When a destructor is accepted, the caller is pushed in the A/S stack instead of
the acceptor. The destructor can resume any blocked tasks on condition queues. 

* What does a task behave like when the main thread finishes?

_Monitor

* When do you accept a stop method vs a destructor?

When you want you convert the Task into a Monitor. Destructor will tear the task
down. If you dont want that, use a stop method.

Internal Scheduling
===================

Use a uCondition lock like before:

Task BoundedBuffer {
	uCondition full, empty;
	int front, back, count;
	int Elements[20];
	public:
	BoundedBuffer() : front(0), back(0), count(0) {}
	Nomutex int query() { return count; }
	void insert( int elem ) {
		if ( count == 20 ) empty.wait();
		Elements[back] = elem;
		back = ( back + 1 ) % 20;
		count += 1;
		full.signal();
	}
	int remove() {
		if ( count == 0 ) full.wait();
		int elem = Elements[front];
		front = ( front + 1 ) % 20;
		count -= 1;
		empty.signal();
		return elem;
	}
	protected:
	void main() {
		for ( ;; ) {
			Accept( ~BoundedBuffer )
			break;
			or Accept( insert, remove );
			// do other work
		} // for
	}
};

* Is there starvation?

No starvation before if there are 50 inserts and 50 removes, then even though we
prioritize inserts, we will service removes when the buffer fills up. Since the
buffer is bounded, there will be cases when it filles up if we are just
accepting inserts. In those cases, inserts will not be accepted and removes will
get service. At that point, it will alternate between inserts and removes. 

Increasing Concurrency
======================

Server side
===========
* How do you increase server side concurrency?

Move as much code into main as possible. This allows the client to make a call
and leave as soon as possible. Then the main function takes over and does the
processing. Meanwhile the client goes and does its thing. This also allows other
clients to queue up to member calls so that when the main is ready to accept
calls again, it already has a client waiting. 

No Conc:
Task server1 {
	public:
	void mem1(. . .) { S1 }
	void mem2(. . .) { S2 }
	void main() {
		. . .
		Accept( mem1 );
		or Accept( mem2 );
	}
}

Some conc by splitting up the work
Task server1 {
	public:
	void mem1(. . .) { S1.part1; }
	void mem2(. . .) { S2.part1; }
	void main() {
		. . .
		Accept( mem1 ) {
			S1.part2;
		} or Accept( mem2 ) {
			S2.part2;
		}
	}
}

Internal Buffer
===============

* How do you increase concurrency even more?

Use an internal buffer like a work queue.

* What are cons of a work queue?

- If the fixed sized work queue is always full or always empty, no gain
- Because of mutex property, clients cant just drop off tasks since blocked when
	main thread is active. 
- Clients may need replies

* What is a worker task?

Its an additional thread on top of the single thread of a task. It allows server
to keep processing but also clients to drop off work.

* Why is it recommended to have same number of workers as clients?

Each client can use a worker and send the request to the server without waiting
on a worker to be available. ie 50 clients : 1 worker vs 50 clients : 25
workers.

Administrator
=============

* Why do we need an admin task?

It is a server managing multiple clients and worker tasks. It does no real work.

* What are the responsibilities of an admin?

- delegate tasks to others
- receive and check complete work
- pass complete work on to waiting tasks

* Why cant an admin make calls to other tasks?

If the admin makes a call, it may block and essentially eliminate the entire
reason for having an admin in the first place. It can block, but not on calls.

It may potentially cause deadlocks.

* When can an admin block?

Waiting for work from clients.

* Why does admin have a list of work?

It will fill this list with requests from clients and take each item and
delegate them to worker tasks.

* What are worker tasks?

Typical workers:

- Timers: prompt the admin at intervals
- Notifier: perform a potentially blocking wait on external event
- Simple worker: do work from admin and return work to admin
- Complex worker: interacts directly with clients
- Courier: perform blocking call on behalf of admin

Client Side
===========

* How can we increase client side concurrency?

We can make it so that the client doesnt wait for the return value. It makes the
call and simply keeps going. Idea of async call.

Returning Values
================

Async calls simply when no returns

* How do you do async in the case of return values?

void run() {
	callee.start(arg); // provide args for the function to run
	// keep working
	result = callee.finish(); // Get the server return or block until available
}

* What is the ticket protocol?

Use tickets. Immediately after a call, a ticket is issued. Second call sends
ticket to get the data.

Cons:
- forged tickets
- multiple use of tickets

* What is the call-back routine protocol?

Register a callback function to the initial func call. When done, callback
called.

Pros:
- server doesnt store result, calls callback when ready
- clients write the cb routine => can decide to block or poll

Cons: 
- client needs to poll to see if the result is ready

Futures
=======

* What is a future?

Future is a value thats returned immediately and client thinks its actual value.
When the server has a return value, it puts in the future. If the future is
accessed before that, it implicitly blocks. 

* How is a future implemented?

Using a semaphore internally and extends the return type.  

* What are the two types of futures?

- Future_ESM<T>
- Future_ISM<T>

* What is implicit storage vs explicit storage for futures?

Explicit is user must dealloc, implicit relies on GC.

* What are advantages of futures?

- No protocols
- Caller doesnt need to poll to see if result is avail
- Single call needed instead of two calls in the case of tickets

* What are the client operations for futures?

Future_ISM<int> f;
f = server.perform(...);

* Whats the client side API?
// f not needed yet
cout << f(); 	// blocks if future not yet ready
cout << f; 		// no more () needed after first time

.available()  // true is future value is ready. Non blocking
.reset() 		  // mark future 
.operator() 	// returns "READ ONLY" copy of result, blocking, raises exception
.operator T 	// casts only allowed after blocking or available returned true
.cancel() 		// cancels work if server allows
.cancelled() 	// returns true if cancelled

* What are the server operations for futures?

.delivery( T result ) 			// delivers the result, unblocks client
.reset() 										// mark future as empty
.exception( uBaseEvent *e ) // throws exception.

* Who deletes the exception? 

Future deletes it implicitly.

_Event E {};
Future ISM<int> result;
result.exception( new E ); // deleted by future when reset or deleted

Complex Future Access
=====================

* What is _Select?

Waits for the future to be available

* How is _Select different from operator()?

operator() throws but select just continues letting us know its available to be
queried. Once queried do exceptions reveal themselves.

* Whats an example _Select?

_Select ( (f1 || ( f2 && f3 ) ) );

Note: If f1 is available, then unblocks

* Whats an advance _Select usable?

When ( conditional-expression ) Select( f1 )
	statement-1 // action
or 
	When ( conditional-expression ) Select( f2 )
		statement-2 // action
	and When ( conditional-expression ) Select( f3 )
		statement-3 // action
















































