Chapter 6
=========

Lock Programming
================

* What is buffering?

When tasks communicate using a queue. Consumers pick up tasks in front of the
queue while producers put tasks at the back.

* How do you control access when using a buffer?

Have a counting semaphore. Initialize it to 0. When producers place thing in,
increase the number by one, ie .V(). When consumers take it, reduce it by 1, ie
.P().

Sample:
void Producer::main(){
	for (;;){
		// prod
		sem.V();
	}
}

void Consumer::main(){
	for(;;){
		sem.P();
		// take an item
	}
}

* Is there a problem when we add/remove from a shared queue?

Yes. Lets say sem = 3 and 2 consumers acquires the lock. They can both remove an
item from the queue. If they go at the same time, they take the same item and
cause issues because it seems like we consumed two items. However, in reality,
we processed just one.

* Is the semaphore above used for ME or sync?

Used for ME because it controls how/who is allowed into the critical section at
any given time.

* Whats bounded vs unbounded?

Bounded has finite length, unlike unbounded.

* What additional mechanism is needed for bounded queue?

We need an additional counting semaphore. The previous was for figuring when the
consumers needed to wait. We now need one when the producers need to wait.

void Producer::main(){
	for (;;){
		sem2.P(); // if the buffer is full, wait
		// prod
		sem.V();
	}
}

void Consumer::main(){
	for(;;){
		sem.P();
		// Took an item, thus, there is one empty slot. Signal prod
		sem2.V();
	}
}

Locking Techniques
==================

* What is a split binary semaphore?

A collection of semaphores where at most one is 1, others are 0. Used when
different tasks block based on their own independent criteria.

* What is baton passing?

Conceptual idea that helps you understand what goes on in the case of split
binary semaphores. Rules are that:

- One baton
- No task moves in the exit/entry code unless they have the baton
- Once baton released, cannot view variables in the entry/exit regions

- Can mutex/condition locks perform baton passing to prevent barging?

Lets say a task is waiting on a condition lock. It gets signalled but the
signaller is still going because they haven't released the lock. The waiting
task is still waiting, and there could be a race with a new task. If the race is
won by the new task, then it just barged in front of the waiting task. Thus,
no.

* What is the reader/writer problem?

Multiple tasks sharing a resource. Some reading others writing. We want to allow
concurrent readers but only one writter.

Using split binary semaphore, we split arrivers, readers and writers.

uSemaphore entry_q(1), read_q(0), write_q(0);
int r_waiting = 0, w_waiting = 0, r_count = 0, w_count = 0;

void Reader::main(){
	// Need access to the entry location since only one task has baton
	entry_q.P();
	if ( w_count > 0 ) { // we have active writer
		r_waiting += 1;
		entry_q.V(); // release the baton
		read_q.P(); // block on the read queue
	}

	r_count += 1; // we are now able to read.

	if ( r_waiting > 0 ) {
		// signal a waiting reader since reading has begun
		r_waiting -= 1;
		read_q.V();
	} else {
		// last waiting reader needs to open up the entry code to both
		// reader and writer
		entry_q.V();
	}

	yield(); // simulate a read

	entry_q.P(); // After done, we need baton to move in the exit code

	r_count -= 1; // we just left, thus one less reader
	if ( r_count == 0 && w_waiting > 0 ) {
		// if writers are waiting, let them in
		w_waiting -= 1;
		write_q.V();
	} else {
		// Otherwise open to all
		entry_q.V();
	}
}

void Writer::main() {
	// Need access to the baton for entry code
	entry_q.P();
	// We need to wait if there is reader OR writer in the room
	if ( r_count > 0 || w_count > ) {
		w_waiting += 1;
		entry_q.V(); // release the baton
		writer_q.P(); // Wait on the writer bench
	}

	w_count += 1;
	entry_q.V(); // release the baton

	if ( r_waiting > 0 ) {
		r_waiting -= 1; read_q.V();
	} else if ( w_waiting > 0 ) {
		w_waiting -= 1; write_q.V();
	} else { entry_q.V(); } // release the baton
}

* What is the problem?

Writers can be starved.

* Whats the solution to starvation?

If we add a || w_waiting > 0 clause in the reader guard, then readers can now
starve.

Use Dekker's algo to alternate when there are ties. If reader goes once, then
next time, force the writer to go. Problem is now staleness and freshness. It
could be that the writer needs to update the values but cant go because writers
are blocked. Meanwhile, readers reading stale data. If there are consecutive
writers then the data is too fresh.

* What is temporal ordering?

Temporal ordering is when the readers and writers are served in FIFO ordering.
We need one semaphore for that otherwise we dont distinguish between reader vs
writer arrival times.

We can use a shadow queue to get the type of task info. For instance, we can
have a shadow queue of:

Shadow queue:
Reader -> Writer -> Writer

Semaphore:
Task -> Task -> Task

* Whats the solution to the reader writer temporal problem?

Solution 5: Have a writer chair. If chair is empty, then unblock tasks in
reader/writer sem. If a writer is unblocked, place on chair.

Solution 6:
===========

Situation
For writer:
- Pick up baton, check reader is using resource
- Put down the baton, entry.V(), then gets time sliced before wait X_p.P()
- Another writer does the same thing
- Writers start at any order => freshness problem

Situation
For reader:
- Pick up baton and see if writer using resource
- Put down the baton, entry.V(), then gets time sliced.
- Happens multiple times => resumption is any order

Thought:
We can have atomic block and release or ticket method
X.P(entry_q); //uC++ semaphore

Ticket solution:
- Reader/writer take ticket before putting baton down
- To pass baton, serving counter is incremented and then wake all blocked
- Each woken one checks to see if their ticket is being served
- No starvation since waiting queue is bounded length and you will be served
- Inefficient AF

Private semaphore solution:
- List of private semaphores, one for each waiting task
- Add to list of nodes before entry.V() then block on private semaphore
- To pass baton V on the private semaphore head if present

Chapter 7
=========

Race Conditions
===============

* What is a race condition and when do they occur?

Race condition occurs when there is a missing sync or mutual exclusion. Two
tasks continue with the assumption that sync took place and depending on
execution speed for each run, outcome will vary and is undeterministic.


Live-lock
=========

* What is livelocking?

Its the indef postponement. Each task says: "You go first" and no one gets to
go.

* What causes live-locking?

It is caused by poor scheduling in entry protocol

Starvation
==========

* What is starvation?

A waiting task never gets serviced due to the selection algorithm

Deadlocking
===========

* What is deadlocking?

A state when one or more process is waiting for an event that will never occur.

* What is the difference between livelocking and deadlocking?

In livelocking, the tasks are always changing states and checking each other for
an update. The CPU is maxed out during this process. In deadlocking, the task is
waiting on some sort of lock and never gets serviced. As a result, the CPU does
no work and idles.

* What is sync deadlock?

Sync deadlock is failure in cooperation. A waiting task is waiting on the
cooperation to finish but never happens.

* What is ME deadlock?

When a task waits on a resource and is unable to acquire a resource thats locked
in ME.

* What are the 5 conditions for deadlock?

1. There is one or more shared resource requiring ME
2. A process goes to sleep while holding a resource (hold and wait)
3. The runtime sys cant forcefully get a held resource of a waiting task
4. There is a circular wait of processes on resources
5. These conditions happen simultaneously

Neat idea: since we need all 5, prevent one and you have no deadlock

* Deadlock Prevention

** Sync Prevention

* How do you prevent sync deadlock?

Get rid of all sync in a program => no communication. All tasks are indep

* How do you prevent ME deadlock?

1. No ME => not realistic
2. No hold and wait. Only give all res or none.
	- Poor res util
	- Starvation possible when a resource set is never avail and someone waits for
		it
3. Allow preemption for runtime sys
4. Dont have circ wait
5. Prevent simultaneous occurrence

* How do you prevent circular wait on a resource:

1. Divide resource to classes R1 ... Rn.
2. Cannot request resource from a class Ri if not holding res from Rj, j >= i.
	=> Have to request resources in increasing order of category number

* Deadlock avoidance

* Bankers algorithm

Show that a safe resource allocation path exists

Allocation graphs
=================

When a resource is allocated, draw line from process to resource.

* How can you determine deadlocks from a graph?

No directed cycle

* How do you use graph reduction to locate deadlocks?

1. Create a graph where resources are duplicated when multiple tasks need it
2. Find a task with only inwards arrows ( it has all the resources )
3. Delete the task ( completed )
4. Flip resource arrows to processes that need it
5. Repeat.

If you can finish, no deadlock. If you get stuck, deadlock

Detection and recovery
======================

* What do you need for deadlock recovery?

Preemption

Chapter 8
=========

Critical Regions
================

* How can language help with critical regions?

Provide REGION statements

* How can nesting result in deadlocks?

VAR x, y: SHARED INTEGER

task1:
REGION x DO
	REGION y DO

	END REGION
END REGION

task2:
REGION y DO
	REGION x DO

	END REGION
END REGION

* Why is simult reads impossible?

Lock needed before any read is done

* Why is REGION statement better?

Compiler can catch it

* Why is this bad?

Since we allow reads from the outside without REGION statements, it can seem
that values are flickering while reading because some task is updating those
values.

* Conditional Critical Regions

REGION v DO
	AWAIT conditional-expression

END REGION

If condition is false, we release the lock and busy wait on the lock

* Monitor

* What is a monitor?

Monitors ensure that shared data and public members are modified in a serial
fashion.

* What is a mutex member?

A member function that allows one active mutex member. There is mutexlock that
is acquired in entry and released when leaving or exception. Recursive entry is
allowed by using a multi acquistion lock.

* What is a destructor?

Destruction is blocked if the thread is active since the entire _Monitor is a
mutex.

* Scheduling ( Sync )

Monitors may need to schedule tasks in a different order than that of arrival.

* What are the types of scheduling?

- External (outside the monitor) using _Accept
- Internal (inside the monitor) using uCondition (benches)

External Scheduling
===================

* How does _Accept statements work?

There is a chair for each accept statement. When the new function calls are
coming in, they check the chairs. If a chair has the same function, we dont
accept those method calls. When the method that it accepted on exits or waits,
the waiting task then goes on.

* How can you accept on a chain on accept calls?

Change chair to a stack

Internal Scheduling
===================

Using a condition lock, we place calls on a waiting queue. When the
task is signalled, it still waits until the signaller exits or waits.

* When do you use internal vs external?

External => Use external scheduling when possible. Cannot use when we need data
from outside to decide when to block/unblock.

Internal => When we want a special ordering of method calls.

Reader/Writer Problem
=====================

* Stalenss in solution p137 but no freshness. Why?

Solution on pg 137 has staleness because the statement:

void endWrite(){
	wcnt = 0;
	if (!readers.empty()) readers.signal();
	else writers.signal();
}

We are prioritizing the readers. If its the other way around, we would freshness.

* Can the monitor read/write have the actual read/write functions in them?

For read: no because we will have no concurrency. One read at a time = stupid
For write: yes because we allow one writer at a time. Monitors will block others.

* What is the problem with this interface?

Forget the endRead() by the dev.

Alternative:

_Monitor ReadersWriter {
	_Mutex void startRead();
	...
	public:
	_Nomutex void read(...) {
		startRead();
		// read
		endRead();
	}
	...
}

Must be a _Nomutex, otherwise only one reader is allowed at a time.

* Using shadow queue to get temporal ordering: (Using shadow queue)

_Monitor ReadersWriter{
	int rcnt, wcnt;
	uCondition RW; // one bench for both
	enum RWenum { READER, WRITER };

	public:
	void startRead() {
		// if writer active or bench not empty, wait because we have to wait for our
		// turn to read
		if (wcnt != 0 || !RW.empty()) { RW.wait( READER ); }
		rcnt += 1;
		// if the front is a reader, then allow them to go. Otherwise, we have to
		// wait for the writer to update the values to prevent stale reads. If we
		// keep allowing readers before allowing that writer, then there is
		// staleness issue.
		if (!RW.empty() && RW.front() == READER) RW.signal();
	}

	// Nothing tricky here
	void endRead() { ... }
	void startWrite() { ... }
	void endWrite() { ... }
}

* Fix to all the issues:

Use external scheduling with _Accept().

* What is the nested monitor problem?

Say you are in Monitor X and you call Monitor Y. You have both X and Y's lock.
If you block inside Y, you implicitly release Y's lock. However, you are still
holding X's lock. If the cooperation to release you requires access to X's
methods, then its a deadlock because no one can enter X.

Condition, Signal, Wait vs Counting Sem, V, P
=============================================

* Whats the differences?

- Wait always blocks but P blocks if sem = 0
- signal before wait is lost, V before P isnt
- Multiple Vs can start multiple functions at once. Multiple signal wakes one at
	a time because tasks must exit the monitor sequentially

Monitor types
=============

* What is explicit scheduling vs implicit?

Explicit: When you as the dev have to schedule manually via accepts / signals
Implicit: When the monitor tasks care of sched for you. Monitors can prioritize
based on calling (C), signalled (W) and signaller (S) queues.

Possibile implicit priorities:
see pg141

* Why do we reject some priorities?

We reject all priorities where the calling task is the highest priority. This
causes starvation since an endless calling will not allow S/W to be serviced.

* How does implicit signalling work?

There is a waitUntil busy wait on the condition.

Example
_Monitor BoundedBuffer {
	public:
	...
	void insert ( ... ) {
		// Busy wait
		waitUntil count != 20; // not in uC++
		...
	}
}

* What are the types of blocking?

Blocking
C < S < W - Priority blocking because after W, S goes. uC++'s signalBlock()
C = S < W - No Priority blocking because after W, S or C could go

Non blocking
C < W < S - Priority nonblocking. Once W is signalled by S, S keeps running
C = W < S - No Prior nonblocking. Once S is done, W or C can go

Quasi blocking
C < W = S - When W is signalled, W or S can go
C = W = S - When W is signalled anyone can go

Immediate Return
C < W - After W is signalled by S, S is done immediately and W runs
C = W - W or C can go

Implicit Signal
C < W - After W is signalled, by the immediate return signal rule, S is done
C = W - C or W can go

* Pros and cons of signal types

Implicit - Good prototyping, bad perf

Immediate return - Not powerful for all cases. Optimizes for the common case of
signal before return.

Quasi - Make cooperation too difficult because a lot of condition checks are
needed

Priority Nonblocking - no barging and optimizes for signal before return. Just
means that if a signal is before a return, it will finish the signaller. If it
was blocking, then the signaller could have easily ended but now gets put on a
queue. Super inefficient.

Priority-blocking - no barging

No-priority non blocking - requires signalled task to recheck the waiting
condition in case of barging task => use while around wait instead of if

No-priority blocking - requires signaller to recheck condition

Java Monitors
=============

* What is synchronized in java?

_Mutex

* What is the blocking type used in java monitors?

No-priority nonblocking

* Whats the issue here:

p144
class Barrier {
	...
	public synchronized void block() {
		count ++;
		if ( count < N ) {
			try { wait (); } catch ( InterruptedException e ) {}
		} else {
			notifyAll();
		}
		count --;
	}
}

Issue is that when the notifyAll is called, one task may go and set the counter
down by one. Another task can go inside the block() and increase the counter by
one and see that its back to N and it notifies all again. Thats an issue because
the previous batch was still being processed.

* How do you fix it?

class Barrier {
	...
	public synchronized void block() {
		count ++;
		if ( count < N ) {
			try { wait (); } catch ( InterruptedException e ) {}
		} else {
			count = 0; // add this
			notifyAll();
		}
	}
}

* Why is it an issue with spurious wakeup?

Note, spurious wake ups dont exist.

Use ticket counter. p145
class Barrier { // monitor
	private int N, count = 0, generation = 0;
	public Barrier( int N ) { this.N = N; }
	public synchronized void block() {
		int mygen = generation;
		count += 1; // count each arriving task
		if ( count < N ) // barrier not full ? => wait
			while ( mygen == generation )
				try { wait(); } catch( InterruptedException e ) {}
		else { // barrier full
			count = 0; // reset count
			generation += 1; // next group
			notifyAll(); // wake all barrier tasks
		}
	}
}

* What is the issue with building condition variables in java with nested
*	monitors?

Deadlock because you release the lock of the immediate sync method but still
hold the lock of the outer sync method(s).

Chapter 9

Direct Communication
====================

* What are the different ways to communicate between tasks?

- Indirect comm: Tasks use monitor to store the input/output of their
	communication
- Direct comm: Tasks directly talk to each other

Task
====

* What is a task?

Task is like a coroutine becaus it has a main. Main has its own state and a
thread begins work in the main as soon as a task is created. Task also provides
ME so only one active thread is allowed in the object. It has its own thread and
stack and can be used for S/ME.

* List of classes and capabilities in uC++:

Thread  Stack  No S/ME   S/ME
-----------------------------------
NO			NO		 class		 monitor
NO			YES		 corout.	 cormonitor
YES			NO		 reject		 reject				// Not possible because uses caller's stk
YES			YES		 reject?	 task

Scheduling
==========

* In a task, why are _Accept statements moved inside of the main instead of the
* indiv routines?

The main function runs until it exits. As a result, no other calls are accepted
if _Accept is not placed in the main().

* Sample _When and _Accept stmt:

_Task BoundedBuffer {
	...
	void main() {
		for ( ;; ) {
			_When (count != 20) _Accept(insert) {
				// code after insert finishes
			} or _When (count != 0) _Accept(remove) {
				// code after remove finishes
			}
		}
	}
}


* What is _Accept( m1, m2 ) s1 equiv to?

_Accept (m1) s1; or _Accept (m2) s1;

* What happens if no _When clause is satisfied?

Behaves like a switch stmt with no match. It keeps going and doesnt block.

* What happens when multiple clauses are satisfied?

Chooses the first _Accept satisfied. Ordering of _Accepts matter.

* Do the above code block of main() with if statments instead of _When.

	void main() {
		for ( ;; ) {
			if ( count > 0 && count < 20) _Accept(insert, remove);
			else if ( count == 0 ) _Accept(insert);
			else if ( count == 20 ) _Accept(remove);
		}
	}

* How many if statements are needed for N accept clauses?

2^N - 1

* When does an _Accept stmt complete?

When the accepted method is done or a wait occurs inside of it.

* What is the _Else clause?

Using the _Else clause, if no accept can be executed immediately, the
terminating else is executed. For instance, if we need to check if the
destructor has been called but dont want to block on it.

* What happens when exception thrown in the members?

Propagates to the caller and a special event is raised in main()
as a uMutexFailure::RendezvousFailure().

try {
	... _Accept(mem);
} catch (uMutexFailure::RendezvousFailure) {
	...
}

Note: _Enable is implicit for RendezvousFailure.

Accepting Destructor
====================

* Why is accepting a destructor different?

When a caller makes a destruction call, it blocks immediately because
deallocations occur in destructors. We dont want to dealloc storage and then do
the _Accept { /* things in here */ }. Thus, the "things in here" runs
immediately and then the destructor call procedes.

When a destructor is accepted, the caller is pushed in the A/S stack instead of
the acceptor. The destructor can resume any blocked tasks on condition queues.

* What does a task behave like when the main thread finishes?

_Monitor

* When do you accept a stop method vs a destructor?

When you want you convert the Task into a Monitor. Destructor will tear the task
down. If you dont want that, use a stop method.

Internal Scheduling
===================

Use a uCondition lock like before:

Task BoundedBuffer {
	uCondition full, empty;
	int front, back, count;
	int Elements[20];
	public:
	BoundedBuffer() : front(0), back(0), count(0) {}
	Nomutex int query() { return count; }
	void insert( int elem ) {
		if ( count == 20 ) empty.wait();
		Elements[back] = elem;
		back = ( back + 1 ) % 20;
		count += 1;
		full.signal();
	}
	int remove() {
		if ( count == 0 ) full.wait();
		int elem = Elements[front];
		front = ( front + 1 ) % 20;
		count -= 1;
		empty.signal();
		return elem;
	}
	protected:
	void main() {
		for ( ;; ) {
			Accept( ~BoundedBuffer )
			break;
			or Accept( insert, remove );
			// do other work
		} // for
	}
};

* Is there starvation?

No starvation before if there are 50 inserts and 50 removes, then even though we
prioritize inserts, we will service removes when the buffer fills up. Since the
buffer is bounded, there will be cases when it filles up if we are just
accepting inserts. In those cases, inserts will not be accepted and removes will
get service. At that point, it will alternate between inserts and removes.

Increasing Concurrency
======================

Server side
===========
* How do you increase server side concurrency?

Move as much code into main as possible. This allows the client to make a call
and leave as soon as possible. Then the main function takes over and does the
processing. Meanwhile the client goes and does its thing. This also allows other
clients to queue up to member calls so that when the main is ready to accept
calls again, it already has a client waiting.

No Conc:
Task server1 {
	public:
	void mem1(. . .) { S1 }
	void mem2(. . .) { S2 }
	void main() {
		. . .
		Accept( mem1 );
		or Accept( mem2 );
	}
}

Some conc by splitting up the work
Task server1 {
	public:
	void mem1(. . .) { S1.part1; }
	void mem2(. . .) { S2.part1; }
	void main() {
		. . .
		Accept( mem1 ) {
			S1.part2;
		} or Accept( mem2 ) {
			S2.part2;
		}
	}
}

Internal Buffer
===============

* How do you increase concurrency even more?

Use an internal buffer like a work queue.

* What are cons of a work queue?

- If the fixed sized work queue is always full or always empty, no gain
- Because of mutex property, clients cant just drop off tasks since blocked when
	main thread is active.
- Clients may need replies

* What is a worker task?

Its an additional thread on top of the single thread of a task. It allows server
to keep processing but also clients to drop off work.

* Why is it recommended to have same number of workers as clients?

Each client can use a worker and send the request to the server without waiting
on a worker to be available. ie 50 clients : 1 worker vs 50 clients : 25
workers.

Administrator
=============

* Why do we need an admin task?

It is a server managing multiple clients and worker tasks. It does no real work.

* What are the responsibilities of an admin?

- delegate tasks to others
- receive and check complete work
- pass complete work on to waiting tasks

* Why cant an admin make calls to other tasks?

If the admin makes a call, it may block and essentially eliminate the entire
reason for having an admin in the first place. It can block, but not on calls.

It may potentially cause deadlocks.

* When can an admin block?

Waiting for work from clients.

* Why does admin have a list of work?

It will fill this list with requests from clients and take each item and
delegate them to worker tasks.

* What are worker tasks?

Typical workers:

- Timers: prompt the admin at intervals
- Notifier: perform a potentially blocking wait on external event
- Simple worker: do work from admin and return work to admin
- Complex worker: interacts directly with clients
- Courier: perform blocking call on behalf of admin

Client Side
===========

* How can we increase client side concurrency?

We can make it so that the client doesnt wait for the return value. It makes the
call and simply keeps going. Idea of async call.

Returning Values
================

Async calls simply when no returns

* How do you do async in the case of return values?

void run() {
	callee.start(arg); // provide args for the function to run
	// keep working
	result = callee.finish(); // Get the server return or block until available
}

* What is the ticket protocol?

Use tickets. Immediately after a call, a ticket is issued. Second call sends
ticket to get the data.

Cons:
- forged tickets
- multiple use of tickets

* What is the call-back routine protocol?

Register a callback function to the initial func call. When done, callback
called.

Pros:
- server doesnt store result, calls callback when ready
- clients write the cb routine => can decide to block or poll

Cons:
- client needs to poll to see if the result is ready

Futures
=======

* What is a future?

Future is a value thats returned immediately and client thinks its actual value.
When the server has a return value, it puts in the future. If the future is
accessed before that, it implicitly blocks.

* How is a future implemented?

Using a semaphore internally and extends the return type.

* What are the two types of futures?

- Future_ESM<T>
- Future_ISM<T>

* What is implicit storage vs explicit storage for futures?

Explicit is user must dealloc, implicit relies on GC.

* What are advantages of futures?

- No protocols
- Caller doesnt need to poll to see if result is avail
- Single call needed instead of two calls in the case of tickets

* What are the client operations for futures?

Future_ISM<int> f;
f = server.perform(...);

* Whats the client side API?
// f not needed yet
cout << f(); 	// blocks if future not yet ready
cout << f; 		// no more () needed after first time

.available()  // true is future value is ready. Non blocking
.reset() 		  // mark future
.operator() 	// returns "READ ONLY" copy of result, blocking, raises exception
.operator T 	// casts only allowed after blocking or available returned true
.cancel() 		// cancels work if server allows
.cancelled() 	// returns true if cancelled

* What are the server operations for futures?

.delivery( T result ) 			// delivers the result, unblocks client
.reset() 										// mark future as empty
.exception( uBaseEvent *e ) // throws exception.

* Who deletes the exception?

Future deletes it implicitly.

_Event E {};
Future ISM<int> result;
result.exception( new E ); // deleted by future when reset or deleted

Complex Future Access
=====================

* What is _Select?

Waits for the future to be available

* How is _Select different from operator()?

operator() throws but select just continues letting us know its available to be
queried. Once queried do exceptions reveal themselves.

* Whats an example _Select?

_Select ( (f1 || ( f2 && f3 ) ) );

Note: If f1 is available, then unblocks

* Whats an advance _Select usable?

When ( conditional-expression ) Select( f1 )
	statement-1 // action
or
	When ( conditional-expression ) Select( f2 )
		statement-2 // action
	and When ( conditional-expression ) Select( f3 )
		statement-3 // action

Optimization
============

* What are the general forms of optimizations?

- Reordering: reorder code / data to increase perf
- Eliding: remove unnecessary data / data access / computation
- Replication: processors/mem/data/code are duplicated because of limitation in
	processing and communication speed

* What are restrictions on optimizations?

- Isomorphic => same output for an input
- Optimizations are env specific

Sequential Optimizations
========================

* What are the two dependencies?

- Data dep
- Control dep

* What is data dep?

1. x = 0;
2. y = x;

ln1 and 2 cannot be reordered because ln2 has a data dep on ln1.

* What is control dep?

1. if ( x == 0 )
2.	 y = 1;

ln 2 has a control dep on ln 1 => cannot be reordered


* What are examples of disjoint operations?

t = x;
s = y;

Can be reordered as they are completely indep.

* What are issues in the code below?

x = 0; // unnecessary, immediate change
x = 3;
for ( int i = 0; i < 10000; i += 1 ); // unnecessary, no loop body
int factorial( int n, int acc ) { // tail recursion
	if (n == 0) return acc;
	return factorial( n - 1, n * acc ); // convert to loop
}

Note: recursion are costly since stack allocations are needed per frame.
Compiler tries to convert to loop.

Memory Hierarchy
================

Cache Review
============

* What is the problem that we are addressing with caches?

CPU is 100x faster than mem and 10,000x faster than disk. We want to use this
power of the CPU.

* How do we address this problem of CPU being faster than disk?

Move data from disk into very fast local memory called registers.

* What is the problem with registers and loading data into them?

We have a lot of data but 6-256 registers to load the data. Not enough registers
for all because ^ registers => ^ cost.

* What is the solution to low register count?

Move frequently accessed data from the disk to registers for as long as
possible. When the usage becomes less frequent, move them back to disk. CPU uses
algorithms such as LRU to rotate data.

* What happens when data is accessed among threads? Whats the solution to this?

Highly accessed data among programs (threads) are not brought to registers.
Solution is to use hardware cache to stage data without pushing to mem. This
also allows the data to be shared among threads since the cache is visible by
all threads.

* What are the responsibilities of a cache?

Load data from main mem in 32/64/128 byte blocks called cache lines. Caches also
hide the slow accessing nature of main mem. The threads only see the cache.

* How does a cache work?

For example, when x is loaded into reg 1, a cache line containing x,y,z are
copied up the mem hierarchy. When cache is full, cache lines such as x,y,z are
evicted. When program ends, flush addresses from mem hier.

Cache Coherence
===============

* Why are there multiple cache levels?

Each level down from the processor has a slower access time but larger capacity.
We do this so that the data thrown out of L1 cache is in L2 and if needed, can
easily be brought to L1 again without going all the way to the slow main mem.

* Why is it necessary to eagerly move reads up the memory hierarchy?



* Why is it advantageous to lazily move writes down the memory hierarchy?

When the processor makes changes to a value, its written to registers/L1. We want
few writes to main mem. When the cache is about to be rotated out or
invalidated, we only then write to the main mem => 1 write to main mem. If the
program is moved from proc 1 to proc x, then registers are invalidated and the
data hier reforms (writes to main mem). The next time its accessed in proc x,
the data will be pulled from main mem which has the correct value in it.

* What is cache coherence?

Hardware protocol of ensuring update of duplicated data in cache.

* What is cache consistency?

Addresses solves the problem of "When do the processors see the updates" =>
bidirectional sync.

* Whats eager/lazy cache consistency?

Eager: updates are seen instantly => complex / slow
Lazy: allows reader to see own write before ack. => stale data

* What is cache thrashing?

Eviction of useful cache values: ie

INITIAL MEMORY PATTERN: A,B,C,D
Requested: A Pattern: A,B,C,D
Requested: B Pattern: B,A,C,D
Requested: C Pattern: C,B,A,D
Requested: D Pattern: D,C,B,A
Requested: E Pattern: E,D,C,B (MISS)
Requested: A Pattern: A,E,D,C (MISS)
Requested: B Pattern: B,A,E,D (MISS)

In this case, if cache lines [x,y,z] and [y,z,a] were in the cache, and x is
updated, then both lines are thrown out. When proc 2 makes a call to y again, it
will be brought back to memory. The act of it bouncing on the L1 and L2 caches
is called cache thrashing.

* What is false sharing?

Thread 1 read/writes x while Thread 2 read/writes y ⇒ no direct shared access,
but indirect sharing as x and y share cache line.

Concurrent Optimizations
========================

* How is concurrent optimizations different from sequential?

Its more restrictive. Identify concurrent regions of code and restrict those
sections only. In concurrent optimizations, the notion of current value is
blurred because race conditions and scheduling can cause different outputs.

* How can sequential optimizations cause failures in conc. code?

Disjoint Problems
=================

Disjoint writes are not allowed:

1. Wx -> Ry allows Ry -> Wx in sequential.

Not allowed in concurrent because Dekker's algo breaks and both are allowed into
the critical section.

2. Rx -> Wy allows Wy -> Rx in sequential.
Consumer breaks in prod/cons problem.

cons:
1 while ( ! Insert ); // R
2 Insert = false;
3 data = Data; // W

cons:
3 data = Data; // W
1 while ( ! Insert ); // R
2 Insert = false;

The latter one allows readering of uninserted values.

Wx -> Wy allows Wy -> Wx in sequential.

Producer breaks.

Prod:
1 Data = i; // W
2 Insert = true; // W

Prod:
2 Insert = true; // W
1 Data = i; // W

In second example, after insert is true, the consumer can immediately go and
read the value before its placed inside.

Eliding Problems
================

Eliding causes issues:
T1:
...
flag = false // write

T2:
reg = flag // one read, aux variable
while ( reg ) {
	...
} // did not see the change of flag values

Replication Problems
====================

* What is double-check locking?

When you have:
if ( ip == NULL ) { // singleton check
	lock.acquire(); // needed to make a new instance of ip
	if ( ip == NULL ) { // check again since it could have been modified
		ip = new int (0);
	}
	lock.release();
}

* How can instruction reordering or caching cause double-check locking to fail?

If we cache the value of ip, and a read is done, the ip value could be stale.
Hence, it would remake another instance.

If there are two threads, and one is making an instance but not done, another
thread can go past the if check and start accessing ip value before its done
instantiating.

Reordering causes issues because

call malloc // new storage address returned in r1
st #0,(r1) // initialize storage
st r1,ip // initialize pointe

call malloc // new storage address returned in r1
st r1,ip // initialize pointe
st #0,(r1) // initialize storage

In the second one, we are using variable before initialization

Memory Model
============

* What is the memory model?

The set of optim. performed by the CPU implicitly.

see pg171

* What is the minimum mem model needed for conc?

We need at least sequential consistency (SC) where it says that reads may be
stale. We can use Dekker / Peterson to have ME.

Preventing Optimization Problems
================================

* What is the root of all opt problems?

Races on shared vars

* What is race free and how does it get enforced?

Shared variables are protected by SME but races can exist in locks. Variables
used in the program has no data race.

* What are two approaches to for lock programmers to cope with races?

- Ad hoc: manually augment program with pragma to prevent compiler opt.
- Formal: Language has mechanisms to abstractly define locations of races occur.

* What is volatile?

Qualifier that forces variables to update quickly. Created for longjmp. Java
volatile => prevents eliding.

Chapter 11
==========

Atomic (Lock Free) Data structures
==================================

* What is lock free?

Critical sections has no locks

* What is a wait free?

It guarantee's progress => no starvation

Compare and Assign Instructions
===============================

* What is the CAA instruc?

int Lock = OPEN; // shared
bool CAA( int &val, int comp, int nval ) {
	// begin atomic
	if (val == comp) {
		val = nval;
		return true;
	}
	return false;
	// end atomic
}

Usage:
void Task::main() { // each task does
	while ( !CAA(Lock,OPEN,CLOSED) ) {}
	// critical section
	Lock = OPEN;
}

Lock Free Stack
===============

* How do you build a stack that is lock free and has push/pop?

class Stack {
	struct Node {
		// data
		Node *next;
	};
	Node *top;
public:
	void push(Node &n);
	Node* pop();
}

void Stack::push (Node &n) {
	// Code for the case where someone else pushes as you do n.next and if stmt.
	// In that case, we will check if the next value that we assigned is still the
	// top, meaning no one else pushed to the top, then we assign the n value to
	// top.
	for (;;) {
		n.next = top;
		if ( CAA(top, n.next, &n) ) break;
	}
}

Node *pop() {
	Node *t;
	for (;;){
		t = top;
		if (t == NULL) break;
		if (CAA(top, t, t->next)) break;
	}
	return t;
}

ABA Problem
===========

* What is the ABA problem?

When we do a pop and set a temp value to top->next, get time sliced. During the
time slice the top is popped and another value. The top is push back on and now
when we continue, we notice that top is still top and pop it off.

Hardware fix
============

* What is the hardware fix to the ABA problem?

Use tickets and wrap top of stack with a header class:

bool CAVD( uint64 t &val, uint64 t &comp, uint64 t nval ) {
	/* begin atomic */
	if ( val == comp ) { // 64-bit compare
		val = nval; // 64-bit assignment
		return true;
	}
	comp = val; // 64-bit assignment
	return false;
	/* end atomic */
}

struct Header { // 64-bit (2 x 32-bit) or 128-bit (2 x 64-bit)
	Node *top; // pointer to stack top
	int count;
};

struct Node {
	// data
	Header next; // pointer to next node / count
};

void push( Header &h, Node &n ) {
	// We need atomic sets because there can be time slicing in asembly level
	CAVD( n.next, n.next, h ); // atomic assignment: n.next = h
	for ( ;; ) { // busy loop
		// If the node's next value is still the top header (which also checks the
		// ticket count) then we know nothing else was pushed and we can simply
		// update the top with (Header){ &n, n.next.count + 1 }. Otherwise, n.next
		// gets the value of the new h as per CAVD and retries again.
		if ( CAVD( h, n.next, (Header){ &n, n.next.count + 1 } ) ) break;
	}
}

Node *pop( Header &h ) {
	Header t;
	CAVD( t, t, h ); // atomic assignment: t = h
	for ( ;; ) { // busy loop
		if ( t.top == NULL ) break; // empty list ?
		if ( CAVD( h, t, (Header){ t.top->next.top, t.count } ) ) break;
	}
	return t.top;
}

* What are the issues with this solution?

Although the problem is almost fixed, we have the issue that the counter is
finite and after a certain number of pushed, it will overflow and screw up. Also
starvation exists in all the solutions.

Hardware and software fix
=========================

* What is a hazard pointer?

Every thread has a list of nodes that its currently accessing. They cannot be
modified by any other threads.

* How do you determine if a node has a reference?

When a node is deleted, you put it on a private list. Then you periodically loop
the hazard pointers of other nodes to see if any has a reference to this node.
If no, free it.

* Locks vs lock-free:

LF: no deadlocks, cannot handle all crit sec, performance is unclear.

Exotic Atomic Instr
===================

* What is load locked (LL)?

Loads a value from memory and sets hardware reservation on the memory that it
just loaded.

* What is the store conditional (SC) instruction?

Writes new value back to original mem location or another mem location. Only
stores if no interrupt, exception or write occurred at LL reservation. Failure
is indicated by setting the register containing the value to be stored to 0.

* How do you implement test/set with LL and SC?

int testSet( int &lock ) { // atomic execution
	int temp = lock; // read
	lock = 1; // write
	return temp; // return previous value
}

testSet: // register $4 contains pointer to lock
	ll $2,($4) // read and lock location
	or $8,$2,1 // set register $8 to 1 (lock | 1)
	sc $8,($4) // attempt to store 1 into lock
	beq $8,$0,testSet // retry if interference between read and write
	j $31 // return previous value in register $2

* Why does LL and SC not suffer from ABA problem?

SC detects a change to top rather than needing indirect ways of checking if the
value on the top changed (CAA).

Node *pop( Header &h ) {
	Node *t, next;
	for ( ;; ) { // busy wait
		t = LL( top );
		if ( t == NULL ) break; // empty list ?
		next = t->next
		if ( SC( top, next ) ) break; // attempt to update top node
	}
	return t;
}

* What is weak LL/SC?

Reservation granularity may be cache line or memory block rather than word.

Concurrency Language
====================

ADA 95
======

* What is when stmt?

external scheduling. Can only contain global vars.

* What is example ada code?

protected type buffer is -- Monitor
	entry insert( elem : in ElemType ) when count < Size is -- mutex member
	begin
		-- add to buffer
		count := count + 1;
	end insert;
	entry remove( elem : out ElemType ) when count > 0 is -- mutex member
	begin
		-- remove from buffer, return via parameter
		count := count - 1;
	end remove;
	private:
		. . . // buffer declarations
		count : Integer := 0;
end buffer;

* What is a select statement?

External sched and only appears in task main.

* Does ada have internal sched?

No.

* How do you build a blocking queue?

use requeue to block on a member mutex of the same obj.

SR/Concurrent C++
=================

* What are the properties?

- Has external sched thru accept
- no condition var or requeue

* What is the when clause?

Allowed to ref caller's args. When multiple avail, select longest waiting task.

select
	accept mem( code : in Integer )
		when code % 2 = 0 do . . . -- accept call with even code
or
	accept mem( code : in Integer )
		when code % 2 = 1 do . . . -- accept call with odd code
end select;

* What is the by clause?

by clause is calculated for each true when clause and the minimum by clause is
selected.

select
	accept mem( code : in Integer )
		when code % 2 = 0 by -code do . . . -- accept call with largest even code
or
	accept mem( code : in Integer )
		when code % 2 = 1 by code do . . . -- accept call with smallest odd code
end select;

* What are drawbacks?

Doesnt work when selection requires multple params.

Java
====

* When does a thread start?

In member run. Requires explicit call to start after thread declaration.

* How is termination sync acheived?

.join()

* How do you get results after a thread finishes?

mytask th = new myTask(. . .); // create and initialized task
th.start(); // start thread
// concurrency
th.join(); // wait for thread termination
a2 = th.result(); // retrieve answer from task object

* What if synchronized members are public?

No way to do _Accept statements. Complex external scheduling with internal
sched.

GO
==

C++11 Concurrency
=================

Concurrency Models
==================

Past Exam Question and Solution
===============================

* Give 2 situations where Nomutex is useful and give examples.

- For read only routines so that writers are not slowed during the reading, ie,
	a buffer size for drawing a graph
- For combining a multi-step protocol into a single routine, where some steps
	need to release the monitor lock, ie, allowing multiple readers for a
	readers-writer lock.

* Using uC++ write the shortest external schedling monitor using the interface
* that implements a counting semaphore:

_Monitor semaphore {
	int count;
public:
	semaphore (int count = 1) : count(count) {};
	void V() {
		count++;
	};
	void P(){
		if (count <= 0) _Accept(V);
		count--;
	};
}

* For the 8 high level abstractions why is one row rejected and another reject
* is in question?

- Row rejected because thread cant exist without a stack.
- Reject? means that if there is no mutex, then all sync and ME needs explicit
	locks to work properly which is too complex.

* Why is it necessary for flag vars when an admin server needs to return an
* exception for a client call?

It allows the admin to raise the exception on the client's stack who is
currently waiting for the result.

* What is the purpose of each future field:

uSemaphore resultAvailable(); // when the client wants the result, and its not
avail, it blocks on this.
future *link; // Allows the admin to chain the futures
ResultType result; // the actual result that gets the value when available

* What is disjoint reordering with respect to optimization?

When the Read/Write occurs on disjoint and unrelated variables, they can be
reordered however you please because one order doesnt affect the other.

example:
int x = 0;
int y = 1;

* How can disjoint reordering cause a concurrent program to fail?

Lets see the case of consumer in a prod/cons problem.

prod:
data = i;
inserted = true;

consumer:
while (!inserted) {}
inserted = false;
data = item;

vs:

consumer:
data = item;
while(!inserted){}

In the latter reordering, we consumed an item that has not been inserted yet.

Concurrency Models
==================

* What is an actor?

An admin accepting messages, handling it or sending it to workers.

* Whats the diff between scala and uC++?

- Dynamic vs static comm
- match message type vs member routine
- GC can detach threads but uC++ must join


Linda
=====

for ( i = 0; i < bufSize; i += 1 ) { // number of buffer elements
	out( "bufferSem" );
}
void producer() {
	for ( int i = 1; i <= 20; i += 1 ) {
		in( "bufferSem" ); // wait for empty buffer slot
		out( "buffer", rand() % 100 ); // insert element into tuple space
	}
	in( "bufferSem" ); // wait for empty buffer slot
	out( "buffer", -1 ); // insert element into tuple space
}
int consumer() {
	int elem;
	for ( ;; ) {
		in( "buffer", ?elem ); // remove element from tuple space
		out( "bufferSem" ); // indicate empty buffer slot
		if ( elem == -1 ) break;
		// consume element
	}
}
